{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":8899335,"datasetId":5350160,"databundleVersionId":9060188}],"dockerImageVersionId":31286,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================\n# EMPLOYEE SALARY PREDICTION USING ML MODELS\n# ==============================================\n\n# Salary prediction determines whether a person earns <=50K or >50K\n# based on demographic and employment-related features.\n\n\n# ==============================================\n# Step 1: Import Libraries\n# ==============================================\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\n\n# ==============================================\n# Step 2: Load Dataset (YOUR CORRECT PATH)\n# ==============================================\n\ndata = pd.read_csv(\n    \"/kaggle/input/datasets/priyamchoksi/adult-census-income-dataset/adult.csv\"\n)\n\n\n# ==============================================\n# Step 3: Data Preprocessing (FIXED VERSION)\n# ==============================================\n\n# Clean column names\ndata.columns = data.columns.str.strip()\n\n# Replace '?' with NaN\ndata.replace(\"?\", np.nan, inplace=True)\n\n# Separate numeric & categorical columns\nnumeric_cols = data.select_dtypes(include=[\"int64\", \"float64\"]).columns\ncategorical_cols = data.select_dtypes(include=[\"object\"]).columns\n\n# Fill missing values correctly\ndata[numeric_cols] = data[numeric_cols].fillna(0)\ndata[categorical_cols] = data[categorical_cols].fillna(\"Unknown\")\n\n# Encode categorical columns safely\nfor col in categorical_cols:\n    le = LabelEncoder()\n    data[col] = le.fit_transform(data[col])\n\n\n# ==============================================\n# Step 4: Split Dataset\n# ==============================================\n\nX = data.drop(\"income\", axis=1)\ny = data[\"income\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n\n# ==============================================\n# Step 5: Random Forest\n# ==============================================\n\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\nrf_pred = rf.predict(X_test)\n\nprint(\"===== Random Forest Classification Report =====\")\nprint(classification_report(y_test, rf_pred))\n\n\n# ==============================================\n# Step 6: Gradient Boosting\n# ==============================================\n\ngb = GradientBoostingClassifier(n_estimators=100, random_state=42)\ngb.fit(X_train, y_train)\n\ngb_pred = gb.predict(X_test)\n\nprint(\"===== Gradient Boosting Classification Report =====\")\nprint(classification_report(y_test, gb_pred))\n\n\n# ==============================================\n# Step 7: AdaBoost\n# ==============================================\n\nab = AdaBoostClassifier(n_estimators=100, random_state=42)\nab.fit(X_train, y_train)\n\nab_pred = ab.predict(X_test)\n\nprint(\"===== AdaBoost Classification Report =====\")\nprint(classification_report(y_test, ab_pred))\n\n\n# ==============================================\n# Step 8: Comparison Table\n# ==============================================\n\nresults = []\n\nresults.append([\n    \"Random Forest\",\n    accuracy_score(y_test, rf_pred),\n    precision_score(y_test, rf_pred),\n    recall_score(y_test, rf_pred),\n    f1_score(y_test, rf_pred)\n])\n\nresults.append([\n    \"Gradient Boosting\",\n    accuracy_score(y_test, gb_pred),\n    precision_score(y_test, gb_pred),\n    recall_score(y_test, gb_pred),\n    f1_score(y_test, gb_pred)\n])\n\nresults.append([\n    \"AdaBoost\",\n    accuracy_score(y_test, ab_pred),\n    precision_score(y_test, ab_pred),\n    recall_score(y_test, ab_pred),\n    f1_score(y_test, ab_pred)\n])\n\ncomparison = pd.DataFrame(\n    results,\n    columns=[\"Algorithm\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n)\n\nprint(\"\\n===== Algorithm Comparison =====\")\nprint(comparison)\n\n\n# ==============================================\n# Step 9: Best Algorithm\n# ==============================================\n\nbest = comparison.loc[comparison[\"F1-Score\"].idxmax()]\n\nprint(\"\\n===== Best Algorithm Based on F1-Score =====\")\nprint(best)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T14:01:45.576418Z","iopub.execute_input":"2026-02-27T14:01:45.577876Z","iopub.status.idle":"2026-02-27T14:01:55.000591Z","shell.execute_reply.started":"2026-02-27T14:01:45.577830Z","shell.execute_reply":"2026-02-27T14:01:54.999009Z"}},"outputs":[],"execution_count":null}]}